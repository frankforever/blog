**Nerual network Basics**

从被提供的数据中学习，被称为自适应学习（adaptive learning），而神经网络创造其自己的组织或信息表征的能力则被称为自组织（self-organisation）。 

1986年，Rumelhart 和 McClelland 编辑的两卷知名著作《Parallel Distributed Processing: Explorations in the Microstructures of Cognition（并行分布式处理：认知的微结构中的探索）》发表。这本书在反向传播的应用上产生了很大的影响，而反向传播也已经在多层感知器的训练上成为了最为流行的学习算法。 



**激活函数**

​	包括：max(0,x)函数，sigmod函数、双曲正切函数、ReLu函数、修正ReLu函数、Maxout函数。

2013 年，Goodfellow 发现使用了一种新的激活函数的 Maxout 网络是 dropout 的天然搭档。

Maxout 单元能促进 dropout 的优化以及提升 dropout 的快速近似模型平均技术（fast approximate model averaging technique）的准确度。单个 Maxout 单元可被理解为是对一个任意凸函数的片（piece）形式的线性近似。

Maxout 网络不仅能学习隐藏单元之间的关系，还能学习每个隐藏单元的激活函数。下面是关于其工作方式的图形化描述： 

![img](C:\project\blog\assets\640) 



**反向传播算法**

​	反向传播算法可用于训练前馈神经网络或多层感知器。这是一种通过改变网络中的权重和偏置来最小化成本函数（cost function）的方法。为了学习和做出更好的预测，会执行一些 epoch（训练周期）；在这些 epoch 中，由成本函数所决定的误差会通过梯度下降被反向传播，直到达到足够小的误差。



**成本函数（Cost Function）** 

​	均方误差函数、交叉熵函数、负对数似然损失（NLL）函 数。



**受限玻尔兹曼机和深度信念网络** 

​	受限玻尔兹曼机（RBM）是一种无监督算法，可被用于预训练深度信念网络。RBM 是玻尔兹曼机的简化版本，受到了统计力学的启发。这种方法是基于给定数据的潜在分布的概率对能量（ energy）建模，这些给定的数据集来自可以派生出的条件分布。 



**DROPOUT**

近期的一些进展已经引入了强大的正则化矩阵（regularizers）来减少神经网络的过拟合。在机器学习中，正则化是附加信息，通常作为一种惩罚机制被引入——惩罚导致过拟合的模型的复杂性。

Dropout 是 Hinton 引入的一种深度神经网络的正则化技术，通过在每一个训练迭代上随机关掉一部分神经元，而是在测试时间使用整个网络（权重按比例缩小），从而防止特征检测器的共适应。

Dropout 通过等同于训练一个共享权重的指数模型减少过拟合。对于给定的训练迭代，存在不同 dropout 配置的不同指数，所以几乎可以肯定每次训练出的模型都不一样。在测试阶段，使用了所有模型的平均值，作为强大的总体方法。